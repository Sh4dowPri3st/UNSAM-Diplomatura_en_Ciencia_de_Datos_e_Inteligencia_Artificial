{"cells":[{"cell_type":"markdown","metadata":{"id":"Mo50gPwW0EKA"},"source":["# Ejercicios Semana 8\n","\n","Para ejercitar Pandas \n"]},{"cell_type":"markdown","source":["## Ejercicio 1:\n","Leer el archivo de datos llamado [estaciones_metereologicas.csv](https://drive.google.com/file/d/1l7H9hdYAzsnXONbmsP2sKb2GRDel80eo/view?usp=share_link) como un un dataframe de Pandas. A partir de este vamos a:\n","\n","1. Mostrar las primeras filas del dataframe.\n","2. Mostrar el número total de filas y columnas.\n","3. Calcular la media, desviación estándar, valor mínimo y valor máximo de la columna `'altura'`."],"metadata":{"id":"k18tvl0TXu0n"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/Programacion/UNSAM-Diplomatura_en_Ciencia_de_Datos_e_Inteligencia_Artificial/1_ProgramacionenPython-Introduccion')\n","\n","import pandas as pd\n","\n","df_estaciones = pd.read_csv('estaciones_metereologicas.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoVLpbZK6Kp3","executionInfo":{"status":"ok","timestamp":1685673407563,"user_tz":180,"elapsed":2716,"user":{"displayName":"Lautaro Bertera","userId":"06509775977976997593"}},"outputId":"f209476a-e7a8-49db-b1a1-b9fdf1fe564c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["df_estaciones.info\n","\n","import numpy as np"],"metadata":{"id":"CR6NmNMj7vEC","executionInfo":{"status":"ok","timestamp":1685656475576,"user_tz":180,"elapsed":6,"user":{"displayName":"Lautaro Bertera","userId":"06509775977976997593"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["numeroFilaColu = df_estaciones.shape\n","print(numeroFilaColu)\n","df_estaciones['ALTURA'].max()\n","df_estaciones['ALTURA'].min()\n","df_estaciones['ALTURA'].mean()\n","df_estaciones['ALTURA'].std()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bzJJYii7IkY","executionInfo":{"status":"ok","timestamp":1685656438492,"user_tz":180,"elapsed":5,"user":{"displayName":"Lautaro Bertera","userId":"06509775977976997593"}},"outputId":"ed0174a2-7298-4b07-d9bb-c2dd0cf07596"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["(123, 9)\n"]},{"output_type":"execute_result","data":{"text/plain":["456.738856510787"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## Ejercicio 2:\n","Dados las siguientes estructuras:  \n","\n","```\n","mi_lista = list('abcdefghijklmnopqrstuvwxyz')\n","\n","mi_array = np.arange(26)\n","\n","mi_diccionario = dict(zip(mi_lista,mi_array))\n","```\n","\n","1. Crear una serie pandas para cada una de ellas, llamadas serLis, serArr y serDic.\n","2. Convertir la serie 'serDic' en un dataframe con su índice como otra columna en el dataframe.\n","3. Combine 'serLis' y 'serArr' para formar un dataframe. \n","4. Exporte el dataframe creado en el punto anterior a un archivo \"serDic.csv\" que quede grabado en la ruta: /content/drive/MyDrive/Colab Notebooks/Data/ (o en alguna equivalente si está trabajando en su propia computadora).\n","\n"],"metadata":{"id":"wTnY9ugUTtNy"}},{"cell_type":"code","source":["mi_lista = list('abcdefghijklmnopqrstuvwxyz')\n","\n","mi_array = np.arange(26)\n","\n","mi_diccionario = dict(zip(mi_lista,mi_array))\n","\n","#1\n","serLis = pd.Series(mi_lista)\n","#print(serLis)\n","serArr = pd.Series(mi_array)\n","#print(serArr)\n","serDic = pd.Series(mi_diccionario)\n","#print(serDic)\n","\n","#2\n","df_lista = pd.DataFrame({'Columna': serLis})\n","#print(df_lista)\n","\n","#3\n","df_lis_arr = pd.DataFrame({'Columna': serLis, 'Columna2': serArr})\n","#print(df_lis_arr)\n","\n","#4\n","serDic.to_csv('serDic.csv', index=True)"],"metadata":{"id":"zukW3cZ19wfL","executionInfo":{"status":"ok","timestamp":1685662873976,"user_tz":180,"elapsed":303,"user":{"displayName":"Lautaro Bertera","userId":"06509775977976997593"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from google.colab import files \n","files.download('serDic.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"9MhXHIzOVlOC","executionInfo":{"status":"ok","timestamp":1685662964596,"user_tz":180,"elapsed":379,"user":{"displayName":"Lautaro Bertera","userId":"06509775977976997593"}},"outputId":"26105f60-1d61-48a6-d02d-f0f4af35f00a"},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_4c1ae030-97c7-4f9e-9ed1-97c9582ac088\", \"serDic.csv\", 123)"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Ejercicio 3:\n","\n","Un conjunto de datos (*dataset*) muy usado para ejercitar `Pandas` es el archivo [titanic.csv](https://drive.google.com/file/d/1edbySvT2KUDelJq85csEA6zxiOJ7TvXH/view?usp=share_link), que contiene información sobre los pasajeros del Titanic, con la descripción de las variables que se puede encontrar [aquí](https://drive.google.com/file/d/14ytgtHkAqbcpbUyWQZkNvw-ff7tisuXK/view?usp=share_link). \n","Para este dataset hacer un programa que:\n","\n","1. Carge los datos en un DataFrame y explorarlos\n","2. Muestre en pantalla las dimensiones, el número de datos que contiene y que muestre los tipos de datos de cada columna. \n","3. Muestre en pantalla los datos del pasajero con identificador 148.\n","4. Encuentre los nombres de las personas que iban en primera clase y los muestre ordenados alfabéticamente.\n","5. Elimine del DataFrame los pasajeros/as con edad desconocida.\n","6. Determine la edad media de las mujeres que viajaban en la clase 1.\n","7. Haga un histograma de la distribución de edades de los sobrevivientes sin distinguir el genero."],"metadata":{"id":"i5WP5pkvZCqH"}},{"cell_type":"code","source":["#1\n","df_titanic = pd.read_csv('titanic.csv')\n","df_titanic.shape\n","#df_titanic.info()\n","#df_titanic.describe()\n","\n","\n","#2\n","#df_titanic.loc[147]\n","\n","#3\n","pasajero148 = df_titanic.loc[df_titanic['IdPasajero'] == 148]\n","#print(pasajero148)\n","\n","#4 \n","primeraClase = df_titanic.loc[df_titanic['Clase'] == 1]\n","primeraClaseORDENADO = primeraClase.sort_values(by='Nombre') \n","primeraClaseORDENADO"],"metadata":{"id":"glwIQ_LbXCoQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejercicio 4:\n","\n","Vamos a trabajar con un nuevo dataset, el del archivo [`\"Spotify 2010 - 2019 Top 100.csv\"`](https://drive.google.com/file/d/18qHLJXvP16kmCfS6ZScAZnBxy1eAs63r/view?usp=sharing), extraído de [aquí](https://www.kaggle.com/datasets/muhmores/spotify-top-100-songs-of-20152019), que contiene información de las canciones más escuchadas cada año en la plataforma de streaming Spotify.\n","\n","\n","1. Explorar el dataset (mirar cuántos líneas hay, qué campos tiene, de que tipo son, etc.)\n","2. Mostrar los 10 artistas con más canciones a lo largo de todos los años y los 10 géneros más frecuentes.\n","3. Buscar cuáles son las canciones con tempos (*bpm*, pulsaciones por minuto) más elevados.\n","(Sugerencia: investigar en la documentación de pandas sobre el método `sort_values()`)\n","4. Crear un nuevo DataFrame con los datos de ritmo (bpm) y duración (dur) promedio para cada año. (Sugerencia: es simple de hacerlo con el método `groupby()`)\n","5. A partir del DataFrame del inciso anterior, haga gráficos de la evolución de la duración y el ritmo promedio a lo largo de los años.\n","6. (Opcional) Investigue cómo hacer gráficos de torta en pandas y haga uno que muestre la proporción de los distintos tipos de artista ('artist type') de todo el dataset."],"metadata":{"id":"Q-DRL8SVVXmU"}},{"cell_type":"code","source":["#df_spotify = pd.read_csv('Spotify 2010 - 2019 Top 100.csv')\n","#1\n","#df_spotify.info()\n","#print(\"\\n\")\n","#print(df_spotify.shape)\n","#print(df_spotify)\n","\n","#2\n","#topArtistas = df_spotify['artist'].value_counts()\n","#print(topArtistas.head(10))\n","\n","print(\"\\n\")\n","\n","#topGenree = df_spotify['top genre'].value_counts()\n","#print(topGenree.head(10))\n","\n","#3\n","#bpmALTO = df_spotify[\"bpm\"].sort_values().tail(10)\n","#print(bpmALTO)\n","#4\n","df_promedio = df_spotify.groupby('year released').mean()[['bpm', 'dur']]\n","df_promedio.columns = ['PromedioBPM', 'PromedioDUR']\n","print(df_promedio)\n","\n","#df_promedio.plot.scatter(x = 'bpm', y = 'dur')\n","\n","#df_promedio.plot.scatter(x = 'year released', y = 'PromedioDUR')"],"metadata":{"id":"92KDnxPlyKcC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejercicio 5:\n","\n","Esta vez vamos a trabajar con los datos de los casos confirmados de Covid en la provincia de Santa Fe, que están en el archivo [`'casos_covid_confirmados_santafe.csv'`](https://drive.google.com/file/d/1DvshOSsVU34KdhCe1k85wyi-KVI9g6QS/view?usp=sharing). Los mismos fueron obtenidos reduciendo el dataset oficial de los casos de covid en todo el país. *(Advertencia: el archivo pesa unos 85 MB)*\n","\n","1. Cargue los contenidos del csv en un DataFrame de Pandas y explore el dataset. En particular, pedimos reportar cuántos registros contiene y qué columnas tiene. ¿Cuáles son todos los valores posibles de la columna `\"clasificacion\"`?\n","2. Calcular cuánta gente de Santa Fe falleció por covid. ¿Qué porcentaje de los casos totales terminaron en muertes (o sea, cuál es la mortalidad)?\n","3. Calcule cuántos casos y muertes hubo en cada departamento de la provincia de Santa Fe.\n","4. Haga un histograma de edades de los casos confirmados. Ojo, hay que descartar primero los que tienen la edad contada en meses (mire la columna `\"edad_años_meses\"`). Haga también un segundo histograma de edades solo de los fallecidos. *(Sí, en el dataset hay alguien que figura que tiene 222 años... esos errores pasan. Se puede intentar filtrar esos casos para que no molesten en el histograma.)*\n","5. Haga el gráfico de casos diarios de covid. Para eso utilizá la columna `\"fecha_apertura\"`.\n","\n"],"metadata":{"id":"xOtFDNzMmv64"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}